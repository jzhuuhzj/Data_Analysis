{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction:\n",
    "- #### 1.1 Objective:Using Borrower’s credit profile, historical credit performance, credit behavior and macroeconomic condition to forecast its potential credit default behavior (Charged Off/Fully Paid) \n",
    "\n",
    "## 2 Data preprocessing:\n",
    "- #### 2.1 Load data and select useful target values\n",
    "- #### 2.2 Feature cleaning and Handle missing value\n",
    "    - ##### Feature cleaning\n",
    "        - ##### 2.2(a) Merge joint and not joint features\n",
    "        - ##### 2.2(b) Covert revol_util to float\n",
    "        - ##### 2.2(c) Add the value of two applicants for some joint feature\n",
    "    - ##### Handle missing vaule\n",
    "        - ##### 2.2(d) Remove meaningless data\n",
    "- #### 2.3 Feature Engineering\n",
    "- #### 2.4 Deal with outlier\n",
    "- #### 2.5 Encode categorial features and feature normalization\n",
    "\n",
    "## 3 Methology:\n",
    "- #### 3.0 train test split and normalization\n",
    "- #### 3.1 Logistic Regression model\n",
    "- #### 3.2 Neural Network Model: MLP\n",
    "- #### 3.3 Random Forest Model\n",
    "- #### 3.4 Decision Tree Model\n",
    "\n",
    "## 4 Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import quandl\n",
    "import quandl\n",
    "\n",
    "quandl.ApiConfig.api_key = '9y_-mxHB3Tj5WboS_z6W'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preprocessing\n",
    "\n",
    "- ### 2.1 Load data and select useful target values\n",
    "Only three loan status are used: “Charged-off”, “Default” and “Fully-paid”.\n",
    "“Charged-off” and “Default” are categorized into the same class “Default”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Read raw data\n",
    "#Load Data from data directory\n",
    "file_name_list = ['LoanStats3a','LoanStats3b','LoanStats3c','LoanStats3d']\n",
    "#                  'LoanStats_2016Q1','LoanStats_2016Q2','LoanStats_2016Q3',\n",
    "#                  'LoanStats_2016Q4','LoanStats_2017Q1','LoanStats_2017Q2',\n",
    "#                  'LoanStats_2017Q3','LoanStats_2017Q4']\n",
    "\n",
    "#Create a raw data dataframe\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "#For all data file in the file_name_list\n",
    "for i in range(len(file_name_list)):\n",
    "    #Read CSV file\n",
    "    #Skip the first row; header=0 denotes the first line of data rather than the first line of the file\n",
    "    temp = pd.read_csv('./data/'+file_name_list[i]+'.csv', \n",
    "                       skiprows=1,header=0, encoding = \"ISO-8859-1\")\n",
    "    #Concatenate all all fles together\n",
    "    raw_data = pd.concat([raw_data, temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a summary of the raw_data and explaination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 144 features in the raw_data\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",raw_data.shape[1],\"features in the raw_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample ratio portion of data, keep the \"Fully Paid\" and \"Default\" entries and remove entires with NaN values\n",
    "- Only keep a random subset of ratio*total_number of rows\n",
    "- Remove some rows with NaN values\n",
    "- Only keep the \"Fully Paid\" and \"Default\" entries\n",
    "- Remove entries with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "samplen = int(ratio*raw_data.shape[0])\n",
    "#Only keep a random subset of ratio*total_number of rows\n",
    "raw_data = raw_data.sample(n=samplen)\n",
    "#re-index the subset\n",
    "raw_data.reindex()\n",
    "\n",
    "raw_data.index = np.arange(raw_data.shape[0])\n",
    "\n",
    "#Remove some rows with NaN values\n",
    "raw_data = raw_data[~raw_data['acc_open_past_24mths'].isna()]\n",
    "raw_data = raw_data[~raw_data['tot_coll_amt'].isna()]\n",
    "\n",
    "#Only keep the \"Fully Paid\" and \"Default\" entries \n",
    "raw_data = raw_data[(raw_data['loan_status']!='Late (31-120 days)') & \n",
    "                    (raw_data['loan_status']!= 'In Grace Period') &\n",
    "                    (raw_data['loan_status']!='Current') &\n",
    "                    (raw_data['loan_status']!='Late (16-30 days)') &\n",
    "                    (~raw_data['loan_status'].isna())]\n",
    "\n",
    "#Remove entries with NaN values\n",
    "raw_data = raw_data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Empty DataFrame"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature cleaning and Handle missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Feature cleaning\n",
    "    - #### 2.2(a) Merge joint and not joint features\n",
    "    Check the existence of \"joint\" features; if so, replace the original feature by the joint one \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process joint features\n",
    "raw_data['annual_inc'] = raw_data['annual_inc'].astype(float)\n",
    "raw_data['annual_inc_joint'] = raw_data['annual_inc_joint'].astype(float)\n",
    "joint_features1 = ['annual_inc','dti','verification_status','revol_bal']\n",
    "for ft in joint_features1:\n",
    "    ft_joint = ft+'_joint'\n",
    "    #Check the existence of \"joint\" features; if so, replace the original feature by the joint one \n",
    "    if ft != 'verification_status':\n",
    "        raw_data[ft] = raw_data.apply(lambda x: x[ft] if math.isnan(x[ft_joint]) else x[ft_joint], axis=1)\n",
    "    else:\n",
    "        raw_data[ft] = raw_data.apply(lambda x: x[ft_joint] if type(x[ft_joint]) is str else x[ft], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - #### 2.2(b) Covert revol_util to float\n",
    "converting from XX% to a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the revol_util feature: converting from XX% to a value between 0 and 1; Note that x[:-1] is to remove the % symbol\n",
    "raw_data['revol_util'] = raw_data['revol_util'].apply(lambda x: float(x[:-1])/100 if type(x) is str else x/100)\n",
    "raw_data['sec_app_revol_util'] = raw_data['sec_app_revol_util']/100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - #### 2.2(c) Add the value of two applicants for some joint feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some features about two applicants, add their values\n",
    "joint_features2 = ['earliest_cr_line','inq_last_6mths','mort_acc','open_acc',\n",
    "                   'revol_util','open_act_il','num_rev_accts',\n",
    "                   'chargeoff_within_12_mths','collections_12_mths_ex_med',\n",
    "                   'mths_since_last_major_derog']\n",
    "for ft in joint_features2:\n",
    "    ft_joint = 'sec_app_'+ft\n",
    "    if ft != 'earliest_cr_line':\n",
    "        raw_data[ft] = raw_data.apply(lambda x: x[ft] if math.isnan(x[ft_joint]) else x[ft]+x[ft_joint], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - #### 2.2(d) Feature cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### meaning for all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             description  \\\n",
      "id                         A unique LC assigned ID for the loan listing.   \n",
      "member_id               A unique LC assigned Id for the borrower member.   \n",
      "loan_amnt              The listed amount of the loan applied for by t...   \n",
      "funded_amnt            The total amount committed to that loan at tha...   \n",
      "funded_amnt_inv        The total amount committed by investors for th...   \n",
      "...                                                                  ...   \n",
      "settlement_status      The status of the borrowerâs settlement plan...   \n",
      "settlement_date        The date that the borrower agrees to the settl...   \n",
      "settlement_amount      The loan amount that the borrower has agreed t...   \n",
      "settlement_percentage  The settlement amount as a percentage of the p...   \n",
      "settlement_term        The number of months that the borrower will be...   \n",
      "\n",
      "                      delete?        delete reason special process method  \\\n",
      "id                        yes         uncorrelated                    NaN   \n",
      "member_id                 yes         uncorrelated                    NaN   \n",
      "loan_amnt                  no                  NaN                    NaN   \n",
      "funded_amnt               yes  information leakage                    NaN   \n",
      "funded_amnt_inv           yes  information leakage                    NaN   \n",
      "...                       ...                  ...                    ...   \n",
      "settlement_status         yes  information leakage                    NaN   \n",
      "settlement_date           yes  information leakage                    NaN   \n",
      "settlement_amount         yes  information leakage                    NaN   \n",
      "settlement_percentage     yes  information leakage                    NaN   \n",
      "settlement_term           yes  information leakage                    NaN   \n",
      "\n",
      "                      categorical  Misssing Percent%  Total missing count  \n",
      "id                             no         100.000000              1237951  \n",
      "member_id                      no         100.000000              1237951  \n",
      "loan_amnt                      no           0.000000                    0  \n",
      "funded_amnt                   NaN           0.000000                    0  \n",
      "funded_amnt_inv               NaN           0.000000                    0  \n",
      "...                           ...                ...                  ...  \n",
      "settlement_status             NaN          97.304821              1204586  \n",
      "settlement_date               NaN          97.304821              1204586  \n",
      "settlement_amount             NaN          97.304821              1204586  \n",
      "settlement_percentage         NaN          97.304821              1204586  \n",
      "settlement_term               NaN          97.304821              1204586  \n",
      "\n",
      "[144 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Read the information about each column/feature from the col_info.csv file\n",
    "col_info = pd.read_csv('./data/data_process.csv', index_col=0, encoding = \"ISO-8859-1\")\n",
    "print(col_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop 'earliest_cr_line','sec_app_earliest_cr_line' features for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['earliest_cr_line' 'sec_app_earliest_cr_line'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8dc5e3e2bc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Drop 'earliest_cr_line','sec_app_earliest_cr_line' features for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#drop the columns in the data; axis=1 means dropping the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'earliest_cr_line'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sec_app_earliest_cr_line'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#drop the columns in the feature description table ; axis=0 means dropping the rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcol_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'earliest_cr_line'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sec_app_earliest_cr_line'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m         )\n\u001b[1;32m   4104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3914\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3946\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5340\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in axis\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['earliest_cr_line' 'sec_app_earliest_cr_line'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#Drop 'earliest_cr_line','sec_app_earliest_cr_line' features for now\n",
    "#drop the columns in the data; axis=1 means dropping the columns\n",
    "raw_data = raw_data.drop(['earliest_cr_line','sec_app_earliest_cr_line'], axis=1)\n",
    "#drop the columns in the feature description table ; axis=0 means dropping the rows\n",
    "col_info = col_info.drop(['earliest_cr_line','sec_app_earliest_cr_line'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove the features that are annotated with \"yes\" in the \"delete?\" column in the col_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = col_info.index[col_info['delete?']=='yes']\n",
    "raw_data = raw_data.drop(drop_list, axis=1)\n",
    "col_info = col_info.drop(drop_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove some joint features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove some joint features\n",
    "drop_list = col_info.index[(col_info['special process method']=='joint-feature') |\n",
    "            (col_info['special process method']=='joint-feature;add to first applicant value') |\n",
    "            (col_info['special process method']=='joint-feature;transform to month until loan issue data, add to first applicant value')]\n",
    "raw_data = raw_data.drop(drop_list, axis=1)\n",
    "col_info = col_info.drop(drop_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing value\n",
    "- ##### 2.2(d) Remove meaningless data\n",
    "    - ##### Remove features if they contain 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove features if they contain 50% missing values \n",
    "drop_list = col_info.index[col_info['Misssing Percent%'] > 50]\n",
    "raw_data = raw_data.drop(drop_list, axis=1)\n",
    "col_info = col_info.drop(drop_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ##### Process the feature according to the instruction  in col_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"if not 0, set as 1\"\n",
    "select_fts = col_info.index[col_info['special process method']==\n",
    "                               'if not 0, set as 1']\n",
    "for ft in select_fts:\n",
    "    raw_data[ft] = raw_data[ft].apply(lambda x: '1' if x!=0 else str(x))\n",
    "\n",
    "#\"if nan, set as 0; else set as 1\"\n",
    "raw_data['emp_title'] = raw_data['emp_title'].apply(lambda x: 0 if type(x) is float else 1)\n",
    "\n",
    "#\"if nan, set as 0; else transform to 1/(1+x)\"\n",
    "select_fts = col_info.index[col_info['special process method']==\n",
    "                                 'if nan, set as 0; else transform to 1/(1+x)']\n",
    "for ft in select_fts:\n",
    "    raw_data[ft] = raw_data[ft].apply(lambda x: 0 if math.isnan(x) else 1/(1+x))\n",
    "\n",
    "#\"if nan use median value of zipcode\"\n",
    "select_fts = col_info.index[col_info['special process method']=='if nan use median value of zipcode']\n",
    "for ft in select_fts:\n",
    "     ft_dict = (raw_data[ft].groupby(by=raw_data.zip_code).median()).to_dict()\n",
    "     raw_data[ft] = raw_data.apply(lambda x: ft_dict[x.zip_code] if math.isnan(x[ft]) else x[ft], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ##### For home ownership features, merge the two infrequent \"ANY\" and \"NONE\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['home_ownership'] = raw_data['home_ownership'].apply(lambda x: 'OTHER'\n",
    "                             if x in ['ANY','NONE'] else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### Remove policy code feature, because all rows have the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove policy code feature, because all rows have the same value\n",
    "raw_data = raw_data.drop(['policy_code'], axis=1)\n",
    "col_info = col_info.drop(['policy_code'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling the missing value from ['LoanStats3a', 'LoanStats3b', 'LoanStats3c', 'LoanStats3d'] ,there are 60 features\n"
     ]
    }
   ],
   "source": [
    "number_of_feature_row = raw_data.shape[1]\n",
    "print(\"After handling the missing value from\",file_name_list,\",there are\",number_of_feature_row,\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a copy of the raw_data named X_loan and remove entries with NaN value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By checking the size of the raw data, there are  76038 rows and 60 columns\n",
      "By checking the size of the X_loan data, there are 69534 rows and 60 columns\n",
      "By checking the size of the col_info, there are 60 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "X_loan = raw_data.copy()\n",
    "#Remove entries with NaN values\n",
    "X_loan.dropna(inplace=True)\n",
    "\n",
    "#Check the sizes of the raw data, the processed data and the feature info\n",
    "print(\"By checking the size of the raw data, there are \",raw_data.shape[0],\"rows and\",raw_data.shape[1],\"columns\")\n",
    "print(\"By checking the size of the X_loan data, there are\",X_loan.shape[0],\"rows and\",X_loan.shape[1],\"columns\")\n",
    "print(\"By checking the size of the col_info, there are\",col_info.shape[0],\"rows and\",col_info.shape[1],\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipcode feature engineering\n",
    "- Map the first three digits to mean household income（x[:3] means the first three digits in the zipcode string）\n",
    "- Add the feature info and index to col_info（Two \"no\"s meaning \"dont delete\" and \"non-categorical\"）\n",
    "- Add the marcoeconomic feature \n",
    "- Add the price level of state info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip2inc(path='./data/ZIP.csv'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        zip_to_inc: dict, map the first three digits to mean household income\n",
    "    \"\"\"\n",
    "    \n",
    "    zip_data = pd.read_csv(path)\n",
    "    zip_data.Zip = zip_data.Zip.apply(lambda x: \"{:0>5}\".format(x)) # 100->00100 padding the zip into 5 space number string\n",
    "    \n",
    "    def helper(x):\n",
    "\n",
    "        x = x.split(',')\n",
    "        new_x = \"\"\n",
    "        for s in x:\n",
    "            new_x += s\n",
    "        \n",
    "        try:\n",
    "            rtn_value = int(new_x)\n",
    "        except ValueError:\n",
    "            rtn_value = new_x\n",
    "            \n",
    "        return rtn_value\n",
    "    \n",
    "    zip_data.Mean = zip_data.apply(lambda x: x.Median if x.Mean=='.' else x.Mean, axis=1)\n",
    "    for ft in ['Mean','Pop']:\n",
    "        zip_data[ft] = zip_data[ft].apply(helper)\n",
    "    \n",
    "    # map the three digit zip code to population-weighted mean household income\n",
    "    zip_data['Zip_3'] = zip_data.Zip.apply(lambda x: x[:3])\n",
    "    \n",
    "    zip_to_inc = {}\n",
    "    for zip_code, df in zip_data.groupby('Zip_3'):\n",
    "        zip_to_inc[zip_code] = (df['Mean'] * df['Pop']).sum() / df['Pop'].sum()\n",
    "    \n",
    "    return zip_to_inc\n",
    "\n",
    "\n",
    "#Start processing the zipcode feature\n",
    "zip_to_inc = get_zip2inc()\n",
    "\n",
    "#Get the first three digits and map to mean household income; x[:3] means the first three digits in the zipcode string\n",
    "#Add to the dataframe\n",
    "X_loan['mean_household_inc'] = X_loan.zip_code.apply(lambda x: zip_to_inc[x[:3]]\n",
    "                                    if x[:3] in zip_to_inc.keys() else np.nan)\n",
    "#Add the feature info and index to col_info\n",
    "#Two \"no\"s meaning \"dont delete\" and \"non-categorical\"\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['mean_household_inc'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### macroeconomic environment feature:\n",
    "- #### Barron's Confidence:\n",
    "yield(AAA) / yiedl(BBB), ML/AAAEY, ML/BBBEY from quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confi_idx(df_loan):\n",
    "    \n",
    "    \"\"\"\n",
    "    MapE the issue_d to the macroeconomic data at that time via quandl.\n",
    "    Barron's Confidence: yield(AAA) / yiedl(BBB), ML/AAAEY, ML/BBBEY from quandl\n",
    "    \n",
    "    Returns:\n",
    "        pandas.Series\n",
    "    \"\"\"\n",
    "    \n",
    "    AAA_yield_data = quandl.get(\"ML/AAAEY\", collapse=\"monthly\", start_date=\"2001-01-01\", end_date=\"2017-12-31\")\n",
    "    BBB_yield_data = quandl.get(\"ML/BBBEY\", collapse=\"monthly\", start_date=\"2001-01-01\", end_date=\"2017-12-31\")\n",
    "    Barron_confi_idx = AAA_yield_data.BAMLC0A1CAAAEY / BBB_yield_data.BAMLC0A4CBBBEY\n",
    "    confi_idx_dict = {}\n",
    "    for d, idx in Barron_confi_idx.iteritems():\n",
    "        str_d = d.strftime(\"%b-%Y\")\n",
    "        confi_idx_dict[str_d] = idx\n",
    "    \n",
    "    return df_loan.issue_d.apply(lambda x: confi_idx_dict[x])\n",
    "\n",
    "#Map issue_d to Barron_confi_idx \n",
    "#Add confi_idx feature to the dataframe \n",
    "X_loan['confi_idx'] = get_confi_idx(X_loan)\n",
    "\n",
    "#Add the feature info and index to col_info\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]], index=['confi_idx'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the price level of state feature and some extra features\n",
    "- RPP(regional price parities) --> the regional price parities according to the state\n",
    "- Satacct_num_to_totacct_num(ratio of satisfactory accounts) --> num of satisfactory/num of all open account\n",
    "- Revbalgt0_num_to_revtrd_num(ratio between the  number of revolving trades with balance --> 0 and the number of currently active revolving trades) --> num_rev_tl_bal_gt_0/num_actv_rev_tl\n",
    "- Extra feature: loan_inc_ratio(the ratio of loan amount to annual income) -->  loan amount / annual income\n",
    "- Extra feature: loan_rev_ratio(the ratio of loan amount to revol_balance) --> loan amount / revol balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the price level of state info from states.csv file\n",
    "states = pd.read_csv(\"./data/states.csv\", index_col=0)\n",
    "abb_to_full = { str(states.loc[idx][0]) : str(idx)  for idx in states.index}\n",
    "\n",
    "rpp_data = pd.read_csv(\"./data/download.csv\", header=4, usecols=['GeoName', 'LineCode', '2010'])\n",
    "rpp_all_data = rpp_data[rpp_data['LineCode'] == 1.0]\n",
    "full_to_rpp = {rpp_all_data.loc[idx, 'GeoName'] : rpp_all_data.loc[idx, '2010'] for idx in rpp_all_data.index}\n",
    "\n",
    "def process_RPP_via_abbre_state(df_loan, abb2full, full2RPP):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process the SARPP Regional Price Parities by states for each loan\n",
    "    The Regional Price Parities measure the price level of the area. \n",
    "    (For the sake of simpilicity, use the 2010 data)\n",
    "    \n",
    "    Source: https://www.bea.gov/data/prices-inflation/regional-price-parities-state-and-metro-area\n",
    "    \n",
    "    Args:\n",
    "        df_loan: pandas.DateFrame, the original data\n",
    "        abb2full: dictionary, map the abbreviation to full name of states, e.g. \"AK\" : \"Alaska\"\n",
    "        full2RPP: dictionary, map the full name of states to the RPP at 2010\n",
    "        \n",
    "    Returns:\n",
    "        RPP: pandas.Series, the regional price parities according to the state   \n",
    "    \"\"\"\n",
    "    \n",
    "    RPP = df_loan.addr_state.apply(lambda x: full2RPP[ abb2full[x] ] )\n",
    "    return RPP\n",
    "\n",
    "#Add the RPP feature to the dataframe\n",
    "X_loan['RPP'] = process_RPP_via_abbre_state(X_loan, abb_to_full, full_to_rpp)\n",
    "#Add the feature info and index to col_info\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]], index=['RPP'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "\n",
    "#Compute the ratio of satisfactory accounts\n",
    "temp = X_loan['num_sats']/X_loan['open_acc']\n",
    "#Deal with inf values\n",
    "temp[(temp==np.inf) | (temp==-np.inf)] = 100\n",
    "#Add the ratio feature to dataframe\n",
    "X_loan['satacct_num_to_totacct_num'] = temp\n",
    "#Add the feature info and index to col_info\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['satacct_num_to_totacct_num'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "\n",
    "#Compute the ratio between the  number of revolving trades with balance > 0 and the number of currently active revolving trades\n",
    "temp = X_loan['num_rev_tl_bal_gt_0']/X_loan['num_actv_rev_tl']\n",
    "#Deal with inf values\n",
    "temp[(temp==np.inf) | (temp==-np.inf)] = 100\n",
    "#Add the ratio feature to dataframe\n",
    "X_loan['revbalgt0_num_to_revtrd_num'] = temp\n",
    "#Add the feature info and index to col_info\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['revbalgt0_num_to_revtrd_num'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "\n",
    "#Compute extra feature: loan amount / annual income\n",
    "temp = X_loan['loan_amnt']/X_loan['annual_inc']\n",
    "temp[(temp==np.inf) | (temp==-np.inf)] = 20\n",
    "X_loan['loan_inc_ratio'] = temp\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['loan_inc_ratio'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "#X_loan['sqrt_loan_inc_ratio'] = np.sqrt(temp)\n",
    "#ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['sqrt_loan_inc_ratio'], columns=col_info.columns)\n",
    "#col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "#X_loan['ln_loan_inc_ratio'] = np.log(temp)\n",
    "#temp[(temp==np.inf) | (temp==-np.inf)] = -20\n",
    "#ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['ln_loan_inc_ratio'], columns=col_info.columns)\n",
    "#col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "##Compute extra feature: 1/dti, sqrt(dti)\n",
    "#X_loan['r_dti'] = 1/X_loan['dti']\n",
    "#temp[(temp==np.inf) | (temp==-np.inf)] = 20\n",
    "#ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['r_dti'], columns=col_info.columns)\n",
    "#col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "#X_loan['sqrt_dti'] = np.sqrt(X_loan['dti'])\n",
    "#ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['sqrt_dti'], columns=col_info.columns)\n",
    "#col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "\n",
    "#Compute extra feature: loan amount / revol balance\n",
    "temp = X_loan['loan_amnt']/X_loan['revol_bal']\n",
    "temp[(temp==np.inf) | (temp==-np.inf)] = 20\n",
    "X_loan['loan_rev_ratio'] = temp\n",
    "ft_inform = pd.DataFrame([[np.nan,'no',np.nan,np.nan,'no',0,0]],index=['loan_rev_ratio'], columns=col_info.columns)\n",
    "col_info = pd.concat([col_info, ft_inform], axis=0)\n",
    "\n",
    "#Drop some useless features\n",
    "drop_list = ['issue_d','zip_code','addr_state']\n",
    "X_loan = X_loan.drop(drop_list, axis=1)\n",
    "col_info = col_info.drop(drop_list, axis=0)\n",
    "X_loan = X_loan.drop(['tot_hi_cred_lim'], axis=1)\n",
    "col_info = col_info.drop(['tot_hi_cred_lim'], axis=0)\n",
    "\n",
    "X_loan.dropna(inplace=True)\n",
    "X_loan.fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate labels for moel trainning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loan = X_loan['loan_status'].apply(lambda x: 0 if x=='Fully Paid' else 1)\n",
    "#Remove label column from data and col_info\n",
    "X_loan = X_loan.drop(['loan_status'], axis=1)\n",
    "col_info = col_info.drop(['loan_status'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding the marcoeconomic environment feature, price level state features and some extra features\n",
      "There are 62 features\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loan_status'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loan_status'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ba7ce59fef11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After adding the marcoeconomic environment feature, price level state features and some extra features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_loan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_loan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loan_status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loan_status'"
     ]
    }
   ],
   "source": [
    "print(\"After adding the marcoeconomic environment feature, price level state features and some extra features\")\n",
    "print(\"There are\",X_loan.shape[1],\"features\")\n",
    "print(X_loan['loan_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 2.5 Encode categorial features and feature normalization\n",
    " - use the function OneHotEncoder from sklearn.preprocessing to encode the categorial features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsorted categories are not supported for numerical categories",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e30d1720139e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_loan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mX_cat_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mXi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                         raise ValueError(\"Unsorted categories are not \"\n\u001b[0m\u001b[1;32m     98\u001b[0m                                          \"supported for numerical categories\")\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhandle_unknown\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsorted categories are not supported for numerical categories"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing  import OneHotEncoder\n",
    "\n",
    "#Find all unique values of each categorical feature\n",
    "#Select categorical features in col_info\n",
    "categorical_list = col_info.index[~(col_info.categorical=='no')]\n",
    "unique_values = []\n",
    "for ft in categorical_list:\n",
    "    unique_values.append(X_loan[ft].unique())\n",
    "\n",
    "# One hot encoding of categorical features\n",
    "enc = OneHotEncoder(categories=unique_values)\n",
    "X_cat = X_loan[categorical_list]\n",
    "enc.fit(X_cat)\n",
    "X_cat_transform = enc.transform(X_cat).toarray()\n",
    "\n",
    "#Add new columns into dataframe\n",
    "col_list = []\n",
    "for i in range(len(categorical_list)):\n",
    "    ft = categorical_list[i]\n",
    "    uniq_vals = unique_values[i]\n",
    "    for j in range(len(uniq_vals)):\n",
    "        col_list.append(ft+'_'+str(uniq_vals[j]))\n",
    "X_cat_transform = pd.DataFrame(X_cat_transform, index=X_loan.index,\n",
    "                               columns=col_list)\n",
    "X_loan = pd.concat([X_loan, X_cat_transform], axis=1)\n",
    "\n",
    "#Remove the original categorical columns\n",
    "X_loan = X_loan.drop(categorical_list, axis=1)\n",
    "\n",
    "print(X_loan.shape)\n",
    "print(y_loan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Methology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 3.0 train test split and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split train/test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_loan, y_loan, test_size = 0.25) \n",
    "\n",
    "def standardize_numerical_features(X, features, n_features, c_features):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use StandardScaler to standardize the numerical features in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        X: pd.DataFrame\n",
    "        features: all features\n",
    "        n_features: list, names of numerical features\n",
    "        c_features: list, names of categorical features\n",
    "        \n",
    "    Returns: \n",
    "        tuple(scaler object, X_std), X_std is a ndarray (numerical, categorical)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Use standardize scaler to transform the numerical features\n",
    "    X = pd.DataFrame(X, columns=features)\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X[n_features])\n",
    "    # Concatenate the numerical and categorical features into ndarray\n",
    "    X_std = np.append(X_std, X[c_features].values, axis=1)\n",
    "    \n",
    "    return (scaler, X_std)\n",
    "\n",
    "def transform_numerical_features(sc, X, features, n_features, c_features):\n",
    "    \n",
    "    \"\"\"\n",
    "    Use the fitted StandardScaler to transform the numerical features of dataset\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(X, columns=features)\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "    X_std = sc.transform(X[n_features])\n",
    "    X_std = np.append(X_std, X[c_features].values, axis=1)\n",
    "    \n",
    "    return X_std\n",
    "\n",
    "\n",
    "\n",
    "#Fit the normalizer only on training data\n",
    "numerical_list = X_loan.columns #col_info.index[col_info.categorical=='no']\n",
    "categorical_list = [] #col_list\n",
    "sc, X_train_std = standardize_numerical_features(X_train, X_loan.columns,\n",
    "                                                 numerical_list, categorical_list)\n",
    "#Perform the scaler on test data\n",
    "X_test_std = transform_numerical_features(sc, X_test, X_loan.columns,\n",
    "                                          numerical_list, categorical_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transfer the training and test dataset into certain type which could fit in the machine learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat_stats(conf_mat, type_str):\n",
    "    TN = conf_mat[0][0]\n",
    "    TP = conf_mat[1][1]\n",
    "    FN = conf_mat[0][1]\n",
    "    FP = conf_mat[1][0]\n",
    "    print(\"In the\"+type_str+\"set, the confusion matrix is: \\n{}\".format(conf_mat))\n",
    "\n",
    "def auc_roc_evaluate(fpr, tpr, type_str):\n",
    "    \n",
    "    from sklearn.metrics import auc\n",
    "    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(\"AUC: \")\n",
    "    print(roc_auc)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: '+type_str)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_test_evaluate(clf):\n",
    "    \n",
    "    from sklearn.metrics import roc_curve\n",
    "    global X_train_std, X_test_std\n",
    "    \n",
    "    y_train_predict = clf.predict(X_train_std)\n",
    "    train_confusion_m = confusion_matrix(y_train, y_train_predict)\n",
    "    train_acc = accuracy_score(y_train, y_train_predict)\n",
    "    conf_mat_stats(train_confusion_m, 'train')\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    y_test_predict = clf.predict(X_test_std)\n",
    "    test_confusion_m = confusion_matrix(y_test, y_test_predict)\n",
    "    test_acc = accuracy_score(y_test, y_test_predict)\n",
    "    conf_mat_stats(test_confusion_m, 'test')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Accuracy: Train:{:.2f}% Test:{:.2f}%\".format(train_acc*100, test_acc*100))\n",
    "\n",
    "    # roc curve    \n",
    "    y_train_prob = clf.predict_proba(X_train_std)\n",
    "    y_test_prob = clf.predict_proba(X_test_std)\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_prob[:,1])\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob[:,1])\n",
    "    \n",
    "    auc_roc_evaluate(fpr_train, tpr_train, 'train')\n",
    "    auc_roc_evaluate(fpr_test, tpr_test, 'test')\n",
    "    \n",
    "    return (y_train_predict, y_test_predict)\n",
    "\n",
    "\n",
    "#Replace inf values; transform numpy array to dataframes\n",
    "X_train_std = pd.DataFrame(X_train_std, columns=X_loan.columns)\n",
    "X_test_std = pd.DataFrame(X_test_std, columns=X_loan.columns)\n",
    "X_train_std.replace([np.inf], 20)                                                                                           \n",
    "X_train_std.replace([-np.inf], -20)                                                                                           \n",
    "X_test_std.replace([np.inf], 20)\n",
    "X_test_std.replace([-np.inf], -20)\n",
    "\n",
    "#Transform dataframes to numpy arrays\n",
    "X_train_std = X_train_std.values\n",
    "X_test_std = X_test_std.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 3.1 Logistic Regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 Regularization hyperparameter\n",
    "parameters = {'C':[0.01, 1]}\n",
    "\n",
    "#Logistic regression classifier with L2, n_jobs is the number of threads used. Use 2 or 4 if running on your laptop\n",
    "lr = LogisticRegression(penalty='l2', n_jobs=40)\n",
    "\n",
    "#Three fold cross-validation for Grid search; verbose=10 meaning output all information\n",
    "gscv_lr = GridSearchCV(lr, parameters, cv=3, verbose=10)\n",
    "gscv_lr.fit(X_train_std, y_train)\n",
    "\n",
    "#Evaluation\n",
    "train_test_evaluate(gscv_lr)\n",
    "print(gscv_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 3.2 Neual Network MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Model: MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(alpha=1)\n",
    "parameters = {'alpha':[0.5,1,2,4], 'solver':['lbfgs'], 'hidden_layer_sizes':[5,10,20]}\n",
    "gscv_mlp = GridSearchCV(MLP, parameters, cv=3, verbose=10)\n",
    "gscv_mlp.fit(X_train_std, y_train)\n",
    "\n",
    "train_test_evaluate(gscv_mlp)\n",
    "print(gscv_mlp.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 3.3 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=15, n_jobs = 40)\n",
    "parameters = {'n_estimators':[200], 'min_samples_leaf':[5], 'max_depth':[10]}\n",
    "gscv_rf = GridSearchCV(RF, parameters, cv=3, verbose=10)\n",
    "gscv_rf.fit(X_train_std, y_train)\n",
    "\n",
    "train_test_evaluate(gscv_rf)\n",
    "print(gscv_rf.best_params_)\n",
    "\n",
    "importances = gscv_rf.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train_std.shape[1]):\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, X_loan.columns[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 3.4 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier(max_depth=15)\n",
    "parameters = {'max_depth':[10,20,30], 'min_samples_leaf':[5,10]}\n",
    "gscv_dt = GridSearchCV(DT, parameters, cv=3, verbose=10)\n",
    "gscv_dt.fit(X_train_std, y_train)\n",
    "\n",
    "train_test_evaluate(gscv_dt)\n",
    "print(gscv_dt.best_params_)\n",
    "\n",
    "importances = gscv_dt.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train_std.shape[1]):\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, X_loan.columns[indices[f]], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1170px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
